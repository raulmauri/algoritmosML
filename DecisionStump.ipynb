{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Stump\n"
      ],
      "metadata": {
        "id": "7WI_9pDly2cV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descripción"
      ],
      "metadata": {
        "id": "LY5HEdJKfhi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt ChatGPT: *describe el algoritmo de ML Decision Stump*"
      ],
      "metadata": {
        "id": "7AQD1D7EyDZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es un modelo de aprendizaje automático que consiste en un árbol de decisión de un nivel. Es decir, es un árbol de decisión con un nodo interno (la raíz) que está conectado inmediatamente a los nodos terminales (sus hojas) Es el tipo de árbol de decisión más simple y se puede utilizar para tareas de clasificación y regresión.\n",
        "\n",
        "En una tarea de clasificación, un decision stump toma una sola característica y la utiliza para dividir un conjunto de datos en dos subconjuntos. Cada subconjunto se asigna a una clase. Por ejemplo, un decision stump podría usarse para clasificar perros y gatos en función de su tamaño. El stump podría dividir el conjunto de datos en dos subconjuntos, uno que contiene perros grandes y uno que contiene gatos grandes.\n",
        "\n",
        "En una tarea de regresión, un decision stump toma una sola característica y la utiliza para predecir un valor. Por ejemplo, un decision stump podría usarse para predecir el precio de una casa en función de su tamaño. El stump podría dividir el conjunto de datos en dos subconjuntos, uno que contiene casas grandes y uno que contiene casas pequeñas. El stump luego podría usar el tamaño de una casa para predecir su precio."
      ],
      "metadata": {
        "id": "aVtTEH1wdxaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fDI-6u8hklc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibtex y Referencias"
      ],
      "metadata": {
        "id": "lFA-FOagcmoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper: https://link.springer.com/content/pdf/10.1007/3-540-57868-4_61.pdf?pdf=inline%20link\n",
        "\n",
        "JJ Oliver, D Hand  (1994). Averaging Over Decision Stumps\n",
        "\n",
        "@inproceedings{oliver1994averaging,\n",
        "  title={Averaging over decision stumps},\n",
        "  author={Oliver, Jonathan J and Hand, David},\n",
        "  booktitle={Machine Learning: ECML-94: European Conference on Machine Learning Catania, Italy, April 6--8, 1994 Proceedings 7},\n",
        "  pages={231--241},\n",
        "  year={1994},\n",
        "  organization={Springer}\n",
        "}"
      ],
      "metadata": {
        "id": "gbYMk6pTeOi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tipo de modelo"
      ],
      "metadata": {
        "id": "_Q6pzdvfbgyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Segun el Tipo de aprendizaje:**\n",
        "\n",
        " *Supervisado*\n",
        "\n",
        "**Segun la Asuncion y parametros en entrenamiento:**\n",
        "\n",
        "*Parametrico*\n",
        "\n",
        "**Segun el Tipo de datos al entranar:**\n",
        "\n",
        "*Offline (batches)*\n",
        "\n",
        "**Resultado de entrenamiento:**\n",
        "\n",
        "*Basado en Modelo*"
      ],
      "metadata": {
        "id": "e-Vyp3KMcLk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmos de entrenamiento"
      ],
      "metadata": {
        "id": "1dd_2UrLcLqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt ChatGPT: *describe el algoritmo de Support Vector Machine*"
      ],
      "metadata": {
        "id": "nLCWN-b4k_B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A continuación, se describen los pasos clave del algoritmo ID3:\n",
        "\n",
        "> Bloc con sangría\n",
        "El algoritmo de entrenamiento que utiliza decision stump es el algoritmo de ganancia de información. Este algoritmo mide la cantidad de información que se gana al dividir un conjunto de datos en dos subconjuntos en función de una característica particular. La característica que proporciona la mayor ganancia de información se selecciona para dividir el conjunto de datos.\n",
        "\n",
        "El algoritmo de ganancia de información funciona de la siguiente manera:\n",
        "\n",
        "Se calcula la entropía del conjunto de datos. La entropía es una medida de la incertidumbre en un conjunto de datos.\n",
        "Se calcula la entropía de cada subconjunto dividido.\n",
        "La ganancia de información es la diferencia entre la entropía del conjunto de datos y la entropía de los subconjuntos divididos.\n",
        "La característica que proporciona la mayor ganancia de información se selecciona para dividir el conjunto de datos.\n",
        "\n",
        "\n",
        "El entrenamiento de un Decision Stump (también conocido como \"clasificador débil\") es bastante simple debido a su estructura extremadamente básica, que consiste en un solo nodo de decisión y dos hojas. El proceso de entrenamiento generalmente se realiza en el contexto de algoritmos de conjunto como AdaBoost, donde varios Decision Stumps se entrenan en secuencia. A continuación, se describe el proceso de entrenamiento típico de un Decision Stump:\n",
        "\n",
        "Supongamos que tienes un conjunto de datos de entrenamiento con características X y etiquetas Y, donde Y es una variable binaria que toma valores 1 o -1 para problemas de clasificación binaria.\n",
        "\n",
        "1. **Selección de la característica y el umbral**:\n",
        "   - Selecciona una característica del conjunto de datos que servirá como la característica de división para el Decision Stump.\n",
        "   - Luego, selecciona un umbral (un valor) que divide los datos en dos grupos, uno para cada hoja del Decision Stump. El umbral puede ser cualquier valor dentro del rango de la característica seleccionada.\n",
        "\n",
        "2. **Cálculo de la predicción**:\n",
        "   - Asigna las etiquetas de clase 1 o -1 a cada una de las dos hojas del Decision Stump, dependiendo de si los valores de la característica seleccionada son mayores o menores (o iguales) al umbral elegido.\n",
        "   - Por ejemplo, si la característica seleccionada es \"Edad\" y el umbral es 30, una hoja podría estar etiquetada como 1 (mayor o igual a 30) y la otra como -1 (menor que 30).\n",
        "\n",
        "3. **Cálculo del error ponderado**:\n",
        "   - Calcula el error ponderado cometido por el Decision Stump en función de su predicción y las etiquetas reales del conjunto de entrenamiento. El error ponderado se usa en algoritmos como AdaBoost para determinar cuán bien se está desempeñando el Decision Stump.\n",
        "\n",
        "4. **Cálculo del peso**:\n",
        "   - Calcula un peso para el Decision Stump en función de su error ponderado. Los Decision Stumps que cometen menos errores tienen un peso mayor.\n",
        "\n",
        "5. **Actualización de los pesos de los ejemplos**:\n",
        "   - Actualiza los pesos de los ejemplos de entrenamiento para que los ejemplos mal clasificados por el Decision Stump tengan mayor peso en la siguiente iteración.\n",
        "\n",
        "6. **Repeticiones**:\n",
        "   - Repite los pasos anteriores para entrenar un conjunto de Decision Stumps débiles en secuencia. Cada nuevo Decision Stump se enfoca en los ejemplos que el conjunto anterior clasificó incorrectamente.\n",
        "\n",
        "7. **Predicción final**:\n",
        "   - Para hacer una predicción en un nuevo ejemplo, cada Decision Stump en el conjunto emite una decisión y se realiza una suma ponderada de las decisiones para determinar la predicción final.\n",
        "\n",
        " Cada Decision Stump es simple por sí mismo, pero cuando se combinan en un conjunto, pueden proporcionar un modelo de clasificación sólido."
      ],
      "metadata": {
        "id": "iC-b8mg2jkHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supuestos y Restricciones"
      ],
      "metadata": {
        "id": "KpFoO6zjkals"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt ChatGPT: *dame los supuestos y restricciones para  el algoritmo Decision Stump*"
      ],
      "metadata": {
        "id": "IlH0wThLxxic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí están los supuestos y restricciones principales del algoritmo Decision Stumps:\n",
        "\n",
        "**Supuestos:**\n",
        "\n",
        "1. **Modelo extremadamente simple**: El Decision Stump es un modelo extremadamente simple que consta de un solo nodo de decisión y dos hojas. Esto significa que solo puede tomar decisiones binarias basadas en una única característica y un umbral.\n",
        "\n",
        "2. **Clasificación binaria**: El Decision Stump es adecuado principalmente para problemas de clasificación binaria, donde se trata de asignar ejemplos a una de dos clases posibles (por ejemplo, sí/no, positivo/negativo).\n",
        "\n",
        "**Restricciones:**\n",
        "\n",
        "1. **Limitación de capacidad**: Debido a su simplicidad, los Decision Stumps tienen una capacidad de modelado limitada. Son inadecuados para problemas complejos en los que se requiere un modelo más sofisticado para capturar relaciones más intrincadas en los datos.\n",
        "\n",
        "2. **Sensibilidad al ruido**: Los Decision Stumps pueden ser sensibles al ruido en los datos de entrenamiento debido a su simplicidad. Pueden ser propensos a sobreajustar los datos de entrenamiento si estos contienen ruido o variabilidad aleatoria.\n",
        "\n",
        "3. **Falta de expresividad**: Debido a que solo pueden tomar decisiones binarias basadas en una sola característica, los Decision Stumps no pueden capturar relaciones no lineales o complejas entre las características y las etiquetas. Esto limita su utilidad en problemas en los que se requiere una representación más rica de los datos.\n",
        "\n",
        "4. **Necesidad de conjuntos o algoritmos de conjunto**: Los Decision Stumps se utilizan típicamente en conjuntos o algoritmos de conjunto como AdaBoost para lograr un mejor rendimiento de clasificación. Por sí solos, pueden ser insuficientes para resolver problemas desafiantes.\n"
      ],
      "metadata": {
        "id": "EyOIEZ9skett"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo en python\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9i7DEJRsock"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Crear un conjunto de datos de ejemplo\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Característica única\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # Etiquetas binarias\n",
        "\n",
        "# Función para entrenar un Decision Stump\n",
        "def train_decision_stump(X, y):\n",
        "    n = len(X)\n",
        "    best_error = float(\"inf\")\n",
        "    best_threshold = None\n",
        "    best_polarity = None\n",
        "\n",
        "    for feature in X:\n",
        "        for polarity in [1, -1]:\n",
        "            threshold = polarity * feature\n",
        "            predictions = np.where(X < threshold, 0, 1)\n",
        "            error = sum(predictions != y)\n",
        "\n",
        "            if error < best_error:\n",
        "                best_error = error\n",
        "                best_threshold = threshold\n",
        "                best_polarity = polarity\n",
        "\n",
        "    return best_threshold, best_polarity\n",
        "\n",
        "# Entrenar el Decision Stump\n",
        "threshold, polarity = train_decision_stump(X, y)\n",
        "\n",
        "# Hacer una predicción en un nuevo ejemplo\n",
        "def predict_decision_stump(x, threshold, polarity):\n",
        "    return 1 if polarity * x >= polarity * threshold else 0\n",
        "\n",
        "new_example = 4.5\n",
        "prediction = predict_decision_stump(new_example, threshold, polarity)\n",
        "\n",
        "print(f\"Predicción para {new_example}: Clase {prediction}\")"
      ],
      "metadata": {
        "id": "3p77aGBsuXmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b6ddb3-49f5-4e1d-f0fd-b1caa0b2c048"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicción para 4.5: Clase 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el conjunto de datos Breast Cancer Wisconsin\n",
        "data = load_breast_cancer()\n",
        "X = data.data[:, 0]  # Usar solo una característica (radio medio)\n",
        "y = data.target  # Clasificar tumores como benignos (0) o malignos (1)\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Función para entrenar un Decision Stump\n",
        "def train_decision_stump(X, y):\n",
        "    n = len(X)\n",
        "    best_error = float(\"inf\")\n",
        "    best_threshold = None\n",
        "    best_polarity = None\n",
        "\n",
        "    # Ordenar las características y las etiquetas\n",
        "    sorted_indices = np.argsort(X)\n",
        "    X_sorted = X[sorted_indices]\n",
        "    y_sorted = y[sorted_indices]\n",
        "\n",
        "    for i in range(n - 1):\n",
        "        threshold = (X_sorted[i] + X_sorted[i + 1]) / 2\n",
        "        predictions = np.where(X < threshold, 0, 1)\n",
        "        error = sum(predictions != y_sorted)\n",
        "\n",
        "        if error < best_error:\n",
        "            best_error = error\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_threshold\n",
        "\n",
        "# Entrenar el Decision Stump en el conjunto de entrenamiento\n",
        "threshold = train_decision_stump(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "def predict_decision_stump(X, threshold):\n",
        "    return np.where(X < threshold, 0, 1)\n",
        "\n",
        "y_pred = predict_decision_stump(X_test, threshold)\n",
        "\n",
        "# Calcular la precisión de las predicciones\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Precisión del Decision Stump: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKW1ULmtElOp",
        "outputId": "401a26bc-31c9-4a67-cd26-962dd64bf7af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión del Decision Stump: 61.40%\n"
          ]
        }
      ]
    }
  ]
}